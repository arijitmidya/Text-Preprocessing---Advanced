{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4dffecc-3bde-417b-bb50-74ef42db1113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\ariji\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7dcd79-68cc-47c1-9c0f-67517d1e9b7f",
   "metadata": {},
   "source": [
    "# Stemming \n",
    "\n",
    "Stemming is a text preprocessing technique that simplifies words by stripping prefixes and suffixes, yielding base forms for effective processing and \n",
    "storage. For instance, the word “running” becomes “run” once we’ve performed stemming. However, while performing this technique, it’s important to \n",
    "note that it can result in inaccuracies and semantic loss, as we’ll get to see. Because of this, even though stemming has advantages like reducing \n",
    "vocabulary, it requires careful application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1a8ae5-a821-4d41-9acd-38d54362554d",
   "metadata": {},
   "source": [
    "### 1. PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bafc331-e40c-4771-814b-0fdc3e93c93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a word list\n",
    "\n",
    "words=[\"eating\",\"eats\",\"eaten\",\"writing\",\"writes\",\"programming\",\"programs\",\"history\",\"finally\",\"finalized\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b90542a8-6608-4828-a20e-2a2a46a3d5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating -----------> eat\n",
      "eats -----------> eat\n",
      "eaten -----------> eaten\n",
      "writing -----------> write\n",
      "writes -----------> write\n",
      "programming -----------> program\n",
      "programs -----------> program\n",
      "history -----------> histori\n",
      "finally -----------> final\n",
      "finalized -----------> final\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "for word in words:\n",
    "    print(word , '----------->' , stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "744e2b86-5c29-4b31-a7e4-af3256c8e6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "congratul\n",
      "sit\n"
     ]
    }
   ],
   "source": [
    "## Some issues with Stemming\n",
    "\n",
    "print(stemmer.stem('congratulations'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83a9060-3f1d-4e34-a8ca-190cd9571d8b",
   "metadata": {},
   "source": [
    "### 2. RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee8c2a57-25b4-4ea3-b0c6-fd3f98c17275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car\n",
      "mas\n",
      "was\n",
      "com\n",
      "agree\n",
      "comput\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "\n",
    "# Instantiate and define\n",
    "stemmer = RegexpStemmer('ing$|s$|e$|able$|' , min=4)\n",
    "\n",
    "print(stemmer.stem('cars'))\n",
    "print(stemmer.stem('mass'))\n",
    "print(stemmer.stem('was'))\n",
    "print(stemmer.stem('coming'))\n",
    "print(stemmer.stem('agreeable'))\n",
    "print(stemmer.stem('compute'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387c29b8-0424-4f94-a5d1-0c6e1057df74",
   "metadata": {},
   "source": [
    "### 3. SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d56cd6d-77a4-4aa0-9222-2891a6a40d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arabic danish dutch english finnish french german hungarian italian norwegian porter portuguese romanian russian spanish swedish\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "print(\" \".join(SnowballStemmer.languages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2765910c-6191-4cf3-a1b4-9c3817714fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autobahn\n",
      "come\n"
     ]
    }
   ],
   "source": [
    "# instantiate and define \n",
    "\n",
    "gstemmer = SnowballStemmer(\"german\")\n",
    "print(gstemmer.stem(\"Autobahnen\"))\n",
    "\n",
    "estemmer = SnowballStemmer(\"english\")\n",
    "print(estemmer.stem(\"coming\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c225551e-7876-403a-b691-cf199cf3a005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating---->eat\n",
      "eats---->eat\n",
      "eaten---->eaten\n",
      "writing---->write\n",
      "writes---->write\n",
      "programming---->program\n",
      "programs---->program\n",
      "history---->histori\n",
      "finally---->final\n",
      "finalized---->final\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"---->\"+estemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f38d294c-552a-4c93-abbc-c83f5c908048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>txt145</td>\n",
       "      <td>The software had a steep learning curve at fir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>txt327</td>\n",
       "      <td>I'm really impressed with the user interface o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>txt209</td>\n",
       "      <td>The latest update to the software fixed severa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>txt825</td>\n",
       "      <td>I encountered a few glitches while using the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>txt878</td>\n",
       "      <td>I was skeptical about trying the software init...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  review_id                                               text\n",
       "0    txt145  The software had a steep learning curve at fir...\n",
       "1    txt327  I'm really impressed with the user interface o...\n",
       "2    txt209  The latest update to the software fixed severa...\n",
       "3    txt825  I encountered a few glitches while using the s...\n",
       "4    txt878  I was skeptical about trying the software init..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the necessary dataset\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/ariji/OneDrive/Desktop/Data/reviews.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02bb91e9-7098-438d-8ba2-0f80015c6094",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball_stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e29f4b0-0631-4229-9f65-10bcad7ff650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     the softwar had a steep learn curv at first, b...\n",
      "1     i'm realli impress with the user interfac of t...\n",
      "2     the latest updat to the softwar fix sever bug ...\n",
      "3     i encount a few glitch while use the software,...\n",
      "4     i was skeptic about tri the softwar initially,...\n",
      "5     the analyt featur have provid us with valuabl ...\n",
      "6     i appreci the regular updat that the softwar r...\n",
      "7     i attend a train session for the software, and...\n",
      "8     the softwar document could be more comprehensi...\n",
      "9     i'v recommend the softwar to colleagu due to i...\n",
      "10    the softwar integr with third-parti plugin has...\n",
      "11    i'm look forward to the upcom releas of the so...\n",
      "12    the user communiti is activ and supportive, ma...\n",
      "13    i'v been use the softwar for a while now, and ...\n",
      "14    the user interfac could use some modernization...\n",
      "15    i went for a run and the softwar did a good jo...\n",
      "Name: stemmed_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['stemmed_text'] = df['text'].apply(lambda x: ' '.join([snowball_stemmer.stem(word) for word in x.split()]))\n",
    "print(df['stemmed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c35e32-adb5-414d-b3c8-715e255a829f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. Comparison of multiple Stemming algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8c186c8-3232-42f4-b817-1582c7acc759",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ariji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Error loading porter_stemmer: Package 'porter_stemmer' not\n",
      "[nltk_data]     found in index\n",
      "[nltk_data] Error loading snowball_tagger: Package 'snowball_tagger'\n",
      "[nltk_data]     not found in index\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ariji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import and download prerequisites\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('porter_stemmer')\n",
    "nltk.download('snowball_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3475237e-4d1b-4fe2-bfb2-b55675b3b64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed words using stemmer 1: ['the', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazi', 'dog', '.']\n",
      "Stemmed words using stemmer 2: ['the', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazi', 'dog', '.']\n"
     ]
    }
   ],
   "source": [
    "# Define a simple paragraph\n",
    "paragraph = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Create a list of stemmers\n",
    "stemmers = [\n",
    "    nltk.PorterStemmer(),\n",
    "    nltk.SnowballStemmer(\"english\"),\n",
    "]\n",
    "\n",
    "# Tokenize the paragraph\n",
    "tokens = nltk.word_tokenize(paragraph)\n",
    "\n",
    "# Stem each token using different stemmers\n",
    "stemmed_words = []\n",
    "for stemmer in stemmers:\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "    stemmed_words.append(stemmed_tokens)\n",
    "\n",
    "# Print the stemmed words for each stemmer\n",
    "for i, stemmed_tokens in enumerate(stemmed_words):\n",
    "    print(f\"Stemmed words using stemmer {i+1}: {stemmed_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "305b284b-204f-4536-8690-c3c5f7e5540b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed words using stemmer 1: ['the', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazi', 'dog', '.', 'the', 'lazi', 'dog', ',', 'feel', 'quit', 'put', 'out', 'by', 'the', 'fox', \"'s\", 'antic', ',', 'decid', 'to', 'chase', 'the', 'fox', 'away', 'from', 'hi', 'favorit', 'nap', 'spot', '.', 'the', 'fox', ',', 'be', 'the', 'mischiev', 'creatur', 'he', 'wa', ',', 'mere', 'laugh', 'and', 'continu', 'to', 'taunt', 'the', 'dog', '.', 'as', 'the', 'sun', 'began', 'to', 'set', ',', 'the', 'exhaust', 'dog', 'final', 'gave', 'up', 'the', 'chase', 'and', 'retreat', 'to', 'hi', 'cozi', 'den', ',', 'vow', 'to', 'get', 'reveng', 'on', 'the', 'peski', 'fox', 'the', 'next', 'day', '.']\n",
      "Stemmed words using stemmer 2: ['the', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazi', 'dog', '.', 'the', 'lazi', 'dog', ',', 'feel', 'quit', 'put', 'out', 'by', 'the', 'fox', \"'s\", 'antic', ',', 'decid', 'to', 'chase', 'the', 'fox', 'away', 'from', 'his', 'favorit', 'nap', 'spot', '.', 'the', 'fox', ',', 'be', 'the', 'mischiev', 'creatur', 'he', 'was', ',', 'mere', 'laugh', 'and', 'continu', 'to', 'taunt', 'the', 'dog', '.', 'as', 'the', 'sun', 'began', 'to', 'set', ',', 'the', 'exhaust', 'dog', 'final', 'gave', 'up', 'the', 'chase', 'and', 'retreat', 'to', 'his', 'cozi', 'den', ',', 'vow', 'to', 'get', 'reveng', 'on', 'the', 'peski', 'fox', 'the', 'next', 'day', '.']\n"
     ]
    }
   ],
   "source": [
    "# Define a complex paragraph\n",
    "\n",
    "paragraph = \"\"\"The quick brown fox jumps over the lazy dog. The lazy dog, feeling quite put out by the fox's antics, decided to chase the fox away \n",
    "from his favorite nap spot. The fox, being the mischievous creature he was, merely laughed and continued to taunt the dog. As the sun began to set, \n",
    "the exhausted dog finally gave up the chase and retreated to his cozy den, vowing to get revenge on the pesky fox the next day.\"\"\"\n",
    "\n",
    "# Create a list of stemmers\n",
    "stemmers = [\n",
    "    nltk.PorterStemmer(),\n",
    "    nltk.SnowballStemmer(\"english\"),\n",
    "]\n",
    "\n",
    "# Tokenize the paragraph\n",
    "tokens = nltk.word_tokenize(paragraph)\n",
    "\n",
    "# Stem each token using different stemmers\n",
    "stemmed_words = []\n",
    "for stemmer in stemmers:\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "    stemmed_words.append(stemmed_tokens)\n",
    "\n",
    "# Print the stemmed words for each stemmer\n",
    "for i, stemmed_tokens in enumerate(stemmed_words):\n",
    "    print(f\"Stemmed words using stemmer {i+1}: {stemmed_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc444d3-07cb-4947-886f-656fe48bfefb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c430e7c3-5572-4f4f-91d2-6d42ae553c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9421be9e-fa80-41d4-baac-c32034f0c993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba22365-931b-4e2b-8e37-f22da66755eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
