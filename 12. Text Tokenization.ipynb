{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "604695c5-590c-48ba-b07c-058831e58429",
   "metadata": {},
   "source": [
    "# Text tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bac7d14-87bd-4a5d-bf5d-021a900b251e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\ariji\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3292be6-092f-418f-970f-99c750e1cba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ariji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# download the necessary NLTK data\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a120d419-afcb-4ac0-a990-adf0456eebce",
   "metadata": {},
   "source": [
    "### 1. Word tokenization\n",
    "\n",
    "Word tokenization is a text transformation technique that divides a sentence or a document into individual words or phrases called tokens. The most \n",
    "common way of tokenizing words is splitting text based on whitespace characters such as spaces and tabs. This technique benefits many natural NLP\n",
    "tasks, such as text classification, sentiment analysis, and machine translation. For example, in English, we use word tokenization to split a sentence \n",
    "like “I love eating pizza” into individual tokens, which are “I,” “love,” “eating,” and “pizza.” Similarly, in languages such as French or German,\n",
    "word tokenization can help identify compound words and other unique linguistic features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdb953a3-bb2b-45d3-a2d4-0ac048d615bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'sentence', 'to', 'be', 'tokenized', '.']\n"
     ]
    }
   ],
   "source": [
    "# example 1 : Tokenize a text\n",
    "\n",
    "text = \"This is a sample sentence to be tokenized.\"\n",
    "tokens = nltk.word_tokenize(text , language='english')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb7dc9af-6c98-4ad6-bc00-6ab6ff55cc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'fills', 'my', 'heart', 'with', 'joy', 'unspeakable', 'to', 'rise', 'in', 'response', 'to', 'the', 'warm', 'and', 'cordial', 'welcome', 'which', 'you', 'have', 'given', 'us', '.', 'I', 'thank', 'you', 'in', 'the', 'name', 'of', 'the', 'most', 'ancient', 'order', 'of', 'monks', 'in', 'the', 'world', ',', 'I', 'thank', 'you', 'in', 'the', 'name', 'of', 'the', 'mother', 'of', 'religions', ',', 'and', 'I', 'thank', 'you', 'in', 'the', 'name', 'of', 'millions', 'and', 'millions', 'of', 'Hindu', 'people', 'of', 'all', 'classes', 'and', 'sects', '.']\n"
     ]
    }
   ],
   "source": [
    "# example 2 : Tokenize a text\n",
    "\n",
    "text = \"\"\"It fills my heart with joy unspeakable to rise in response to the warm and cordial welcome which you have given us. I thank you in the name \n",
    "of the most ancient order of monks in the world, I thank you in the name of the mother of religions, and I thank you in the name of millions and \n",
    "millions of Hindu people of all classes and sects.\"\"\"\n",
    "\n",
    "tokens = nltk.word_tokenize(text , language='english')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bba9f883-81ce-43ad-bf8a-bd276ac3b91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   review_id                                               text\n",
      "0     txt145  The software had a steep learning curve at fir...\n",
      "1     txt327  I'm really impressed with the user interface o...\n",
      "2     txt209  The latest update to the software fixed severa...\n",
      "3     txt825  I encountered a few glitches while using the s...\n",
      "4     txt878  I was skeptical about trying the software init...\n",
      "5     txt933  The analytics features have provided us with v...\n",
      "6     txt718  I appreciate the regular updates that the soft...\n",
      "7     txt316  I attended a training session for the software...\n",
      "8     txt247  The software documentation could be more compr...\n",
      "9     txt515  I've recommended the software to colleagues du...\n",
      "10    txt913  The software integration with third-party plug...\n",
      "11    txt341  I'm looking forward to the upcoming release of...\n",
      "12    txt943  The user community is active and supportive, m...\n",
      "13    txt688  I've been using the software for a while now, ...\n",
      "14    txt136  The user interface could use some modernizatio...\n",
      "15    txt137  I went for a run and the software did a good j...\n"
     ]
    }
   ],
   "source": [
    "# example 3\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "df = pd.read_csv(\"C:/Users/ariji/OneDrive/Desktop/Data/reviews.csv\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9df4633-1650-40d8-85bc-3eea7544f19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     [The, software, had, a, steep, learning, curve...\n",
      "1     [I, 'm, really, impressed, with, the, user, in...\n",
      "2     [The, latest, update, to, the, software, fixed...\n",
      "3     [I, encountered, a, few, glitches, while, usin...\n",
      "4     [I, was, skeptical, about, trying, the, softwa...\n",
      "5     [The, analytics, features, have, provided, us,...\n",
      "6     [I, appreciate, the, regular, updates, that, t...\n",
      "7     [I, attended, a, training, session, for, the, ...\n",
      "8     [The, software, documentation, could, be, more...\n",
      "9     [I, 've, recommended, the, software, to, colle...\n",
      "10    [The, software, integration, with, third-party...\n",
      "11    [I, 'm, looking, forward, to, the, upcoming, r...\n",
      "12    [The, user, community, is, active, and, suppor...\n",
      "13    [I, 've, been, using, the, software, for, a, w...\n",
      "14    [The, user, interface, could, use, some, moder...\n",
      "15    [I, went, for, a, run, and, the, software, did...\n",
      "Name: word_tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['word_tokens'] = df['text'].apply(word_tokenize) \n",
    "print(df['word_tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae333506-e09c-43bd-8731-bc6b286435fc",
   "metadata": {},
   "source": [
    "### 2. Sentence tokenization\n",
    "\n",
    "Sentence tokenization is a text transformation technique that involves breaking down a larger text into smaller, individual sentences. We use this\n",
    "technique in NLP applications, such as sentiment analysis and machine translation, to help better understand the meaning and structure of text. For \n",
    "example, a paragraph of text such as “The cat sat on the mat. The dog barked loudly. The sun was shining brightly.” can be divided into\n",
    "three individual sentences:\n",
    "\n",
    "“The cat sat on the mat.”\n",
    "\n",
    "“The dog barked loudly.”\n",
    "\n",
    "“The sun was shining brightly.”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3918855-1f57-4467-98fc-ecf42fef758f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a sentence.', 'This is another sentence.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ariji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# example 1 : Tokenize a text\n",
    "\n",
    "# Download the Punkt tokenizer data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenize a text into sentences\n",
    "text = \"This is a sentence. This is another sentence.\"\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e873a9f-0932-4890-b8c9-991489eee257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It fills my heart with joy unspeakable to rise in response to the warm and cordial welcome which you have given us.', 'I thank you in the name \\nof the most ancient order of monks in the world, I thank you in the name of the mother of religions, and I thank you in the name of millions and \\nmillions of Hindu people of all classes and sects.']\n"
     ]
    }
   ],
   "source": [
    "# example 2 : Tokenize a text\n",
    "\n",
    "text = \"\"\"It fills my heart with joy unspeakable to rise in response to the warm and cordial welcome which you have given us. I thank you in the name \n",
    "of the most ancient order of monks in the world, I thank you in the name of the mother of religions, and I thank you in the name of millions and \n",
    "millions of Hindu people of all classes and sects.\"\"\"\n",
    "\n",
    "sentences = nltk.sent_tokenize(text , language='english')\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c96a0b57-ba35-4553-84a3-88ccef3bd3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   review_id                                               text\n",
      "0     txt145  The software had a steep learning curve at fir...\n",
      "1     txt327  I'm really impressed with the user interface o...\n",
      "2     txt209  The latest update to the software fixed severa...\n",
      "3     txt825  I encountered a few glitches while using the s...\n",
      "4     txt878  I was skeptical about trying the software init...\n",
      "5     txt933  The analytics features have provided us with v...\n",
      "6     txt718  I appreciate the regular updates that the soft...\n",
      "7     txt316  I attended a training session for the software...\n",
      "8     txt247  The software documentation could be more compr...\n",
      "9     txt515  I've recommended the software to colleagues du...\n",
      "10    txt913  The software integration with third-party plug...\n",
      "11    txt341  I'm looking forward to the upcoming release of...\n",
      "12    txt943  The user community is active and supportive, m...\n",
      "13    txt688  I've been using the software for a while now, ...\n",
      "14    txt136  The user interface could use some modernizatio...\n",
      "15    txt137  I went for a run and the software did a good j...\n"
     ]
    }
   ],
   "source": [
    "# example 3\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/ariji/OneDrive/Desktop/Data/reviews.csv\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "026067b6-bf1c-4cb8-9540-564f52604663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     [The software had a steep learning curve at fi...\n",
      "1     [I'm really impressed with the user interface ...\n",
      "2     [The latest update to the software fixed sever...\n",
      "3     [I encountered a few glitches while using the ...\n",
      "4     [I was skeptical about trying the software ini...\n",
      "5     [The analytics features have provided us with ...\n",
      "6     [I appreciate the regular updates that the sof...\n",
      "7     [I attended a training session for the softwar...\n",
      "8     [The software documentation could be more comp...\n",
      "9     [I've recommended the software to colleagues d...\n",
      "10    [The software integration with third-party plu...\n",
      "11    [I'm looking forward to the upcoming release o...\n",
      "12    [The user community is active and supportive, ...\n",
      "13    [I've been using the software for a while now,...\n",
      "14    [The user interface could use some modernizati...\n",
      "15    [I went for a run and the software did a good ...\n",
      "Name: sentence_tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['sentence_tokens'] = df['text'].apply(sent_tokenize) \n",
    "print(df['sentence_tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ad9744-59e7-4c1e-86e9-2ab67b7fd9a6",
   "metadata": {},
   "source": [
    "### 3. Character level tokenization\n",
    "\n",
    "Character tokenization is a text transformation technique that divides text into individual or group characters. Unlike other types of tokenization\n",
    "that split text into words or phrases, character tokenization treats each character as a separate token. This technique is essential when working with \n",
    "languages that do not use spaces between words or when analyzing text at a more granular level. For example, we use character tokenization in Chinese \n",
    "or Japanese to break down text into individual characters, which can help analyze the language’s structure and identify specific \n",
    "characters or patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a648c47-2b3e-4498-99d9-8b17e54fbb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'e', 'l', 'l', 'o', ',', ' ', 'w', 'o', 'r', 'l', 'd', '!']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world!\"\n",
    "\n",
    "# Character-level tokenization\n",
    "tokens = list(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "892c8c12-b75c-4914-a4d5-dc507e0011ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   review_id                                               text\n",
      "0     txt145  The software had a steep learning curve at fir...\n",
      "1     txt327  I'm really impressed with the user interface o...\n",
      "2     txt209  The latest update to the software fixed severa...\n",
      "3     txt825  I encountered a few glitches while using the s...\n",
      "4     txt878  I was skeptical about trying the software init...\n",
      "5     txt933  The analytics features have provided us with v...\n",
      "6     txt718  I appreciate the regular updates that the soft...\n",
      "7     txt316  I attended a training session for the software...\n",
      "8     txt247  The software documentation could be more compr...\n",
      "9     txt515  I've recommended the software to colleagues du...\n",
      "10    txt913  The software integration with third-party plug...\n",
      "11    txt341  I'm looking forward to the upcoming release of...\n",
      "12    txt943  The user community is active and supportive, m...\n",
      "13    txt688  I've been using the software for a while now, ...\n",
      "14    txt136  The user interface could use some modernizatio...\n",
      "15    txt137  I went for a run and the software did a good j...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/ariji/OneDrive/Desktop/Data/reviews.csv\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01261a7a-044d-4131-a9c1-2708a0f605df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     [T, h, e,  , s, o, f, t, w, a, r, e,  , h, a, ...\n",
      "1     [I, ', m,  , r, e, a, l, l, y,  , i, m, p, r, ...\n",
      "2     [T, h, e,  , l, a, t, e, s, t,  , u, p, d, a, ...\n",
      "3     [I,  , e, n, c, o, u, n, t, e, r, e, d,  , a, ...\n",
      "4     [I,  , w, a, s,  , s, k, e, p, t, i, c, a, l, ...\n",
      "5     [T, h, e,  , a, n, a, l, y, t, i, c, s,  , f, ...\n",
      "6     [I,  , a, p, p, r, e, c, i, a, t, e,  , t, h, ...\n",
      "7     [I,  , a, t, t, e, n, d, e, d,  , a,  , t, r, ...\n",
      "8     [T, h, e,  , s, o, f, t, w, a, r, e,  , d, o, ...\n",
      "9     [I, ', v, e,  , r, e, c, o, m, m, e, n, d, e, ...\n",
      "10    [T, h, e,  , s, o, f, t, w, a, r, e,  , i, n, ...\n",
      "11    [I, ', m,  , l, o, o, k, i, n, g,  , f, o, r, ...\n",
      "12    [T, h, e,  , u, s, e, r,  , c, o, m, m, u, n, ...\n",
      "13    [I, ', v, e,  , b, e, e, n,  , u, s, i, n, g, ...\n",
      "14    [T, h, e,  , u, s, e, r,  , i, n, t, e, r, f, ...\n",
      "15    [I,  , w, e, n, t,  , f, o, r,  , a,  , r, u, ...\n",
      "Name: character_tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['character_tokens'] = df['text'].apply(list)\n",
    "print(df['character_tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8741f7a-f1a5-4b99-b513-14a439577f69",
   "metadata": {},
   "source": [
    "### 4. Syntactic tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca2efb0a-f456-4d82-86f7-9bb20857186b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ariji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ariji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Tokenize text into sentences\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "# Tokenize sentences into words and part-of-speech tags\n",
    "for sentence in sentences:\n",
    "  words = nltk.word_tokenize(sentence)\n",
    "  pos_tags = nltk.pos_tag(words)\n",
    "  print(pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fea924-780a-4418-bc2c-b5316a86fd08",
   "metadata": {},
   "source": [
    "### 5. Tokenize and detokenize using treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8138e10e-f1c8-45f2-9993-15e63cf722db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here are my tokens : ['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York.', 'Please', 'buy', 'me', 'two', 'of', 'them.', 'Thanks', '.']\n",
      "Good muffins cost $3.88 in New York. Please buy me two of them. Thanks.\n"
     ]
    }
   ],
   "source": [
    "# example 1\n",
    "\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer, TreebankWordDetokenizer\n",
    "\n",
    "s = '''Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\nThanks.'''\n",
    "d = TreebankWordDetokenizer()\n",
    "t = TreebankWordTokenizer()\n",
    "\n",
    "tokens = t.tokenize(s)\n",
    "print('here are my tokens :' , tokens)\n",
    "\n",
    "print(d.detokenize(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d93a4a8-e0bb-45c3-b3ab-1516e7d5dbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, i can't feel my feet! Help!!\n",
      "hello, i can't feel; my feet! Help!! He said: Help, help?!\n"
     ]
    }
   ],
   "source": [
    "# example 2\n",
    "\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "tokens = ['hello', ',', 'i', 'ca', \"n't\", 'feel', 'my', 'feet', '!', 'Help', '!', '!']\n",
    "d = TreebankWordDetokenizer()\n",
    "print(d.detokenize(tokens))\n",
    "\n",
    "tokens = ['hello', ',', 'i', \"can't\", 'feel', ';', 'my', 'feet', '!', 'Help', '!', '!', 'He', 'said', ':', 'Help', ',', 'help', '?', '!']\n",
    "print(d.detokenize(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fec593-350c-4f91-a78d-bc3c198490cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. Tokenize using regexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c322a8b-d096-4656-944d-6aca89573bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n",
      "['Good', 'muffins', 'cost', '$3.88', 'in', 'New', 'York.', 'Please', 'buy', 'me', 'two', 'of', 'them.', 'Thanks.']\n",
      "['Good', 'New', 'York', 'Please', 'Thanks']\n",
      "['ood', 'muffins', 'cost', 'in', 'ew', 'ork', 'lease', 'buy', 'me', 'two', 'of', 'them', 'hanks']\n"
     ]
    }
   ],
   "source": [
    "# examples\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "s = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\"\n",
    "\n",
    "# 1\n",
    "tokenizer = RegexpTokenizer(r'\\w+|\\$[\\d\\.]+|\\S+')\n",
    "print(tokenizer.tokenize(s))\n",
    "\n",
    "# 2\n",
    "tokenizer = RegexpTokenizer(r'\\s+', gaps=True)\n",
    "print(tokenizer.tokenize(s))\n",
    "\n",
    "# 3\n",
    "capword_tokenizer = RegexpTokenizer(r'[A-Z]\\w+')\n",
    "print(capword_tokenizer.tokenize(s))\n",
    "\n",
    "# 4\n",
    "smallword_tokenizer = RegexpTokenizer(r'[a-z]\\w+')\n",
    "print(smallword_tokenizer.tokenize(s))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a6a22e-5768-4ae8-8d98-a4faef26e1e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e419c30-373d-411d-8ff6-0b8a5b22f287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbdee04-9942-44ae-8504-a90f5305face",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077032a7-d11d-427c-8b99-6122e11f4036",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873c1938-e02f-41f1-87b7-7a95b2704fa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c56243-dbe2-43e7-9555-751c486c1a36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee43d8ad-8c9d-49ce-a18f-277e43f3aee6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a5dc90-1ddb-47c8-a688-614028b1469c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12645d4d-d699-4e1b-a780-ed7dfc3c0cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc51ed6-1215-4f58-b06c-f774da11bc7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aee5108-ffff-4dbe-b63a-f35cfefe8f89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36af424-59a9-493e-bb01-f54d8d45d354",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
