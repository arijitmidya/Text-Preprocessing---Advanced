{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc5e9fbf-8c3b-4c6c-abe6-920d2cdf0eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\ariji\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601a2b49-07ca-4f29-b2b1-6ec4947d9230",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "\n",
    "We use the lemmatization technique over stemming to maintain word meaning and context. Lemmatization is a text preprocessing technique that transforms\n",
    "words into their base or dictionary form, known as lemmas, ensuring a more accurate semantic representation. It considers a word’s grammatical context \n",
    "and part of speech and, as a result, produces meaningful results. For instance, both “jumps” and “jumping” will be lemmatized to “jump,” preserving \n",
    "semantic integrity. However, while this technique offers an enhanced approach, it can be computationally more intensive than stemming. We can perform \n",
    "lemmatization using the NLTK and spaCy libraries. However, as we’ll see, NLTK’s lemmatization might not handle all cases as effectively as spaCy’s \n",
    "lemmatization due to its advanced contextual understanding capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea1139d-b015-41c3-bc1e-d89b9ae3e122",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. WordNet Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "961dcb34-de08-44da-ae77-0eed7798c19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n",
      "church\n",
      "aardwolf\n",
      "====================================================================================================\n",
      "eating---->eat\n",
      "eats---->eat\n",
      "eaten---->eat\n",
      "writing---->write\n",
      "writes---->write\n",
      "programming---->program\n",
      "programs---->program\n",
      "history---->history\n",
      "finally---->finally\n",
      "finalized---->finalize\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer as wnl\n",
    "\n",
    "# example 1\n",
    "print(wnl().lemmatize('dogs'))\n",
    "print(wnl().lemmatize('churches'))\n",
    "print(wnl().lemmatize('aardwolves'))\n",
    "\n",
    "print(\"=\"*100)\n",
    "\n",
    "# example 2\n",
    "words=[\"eating\",\"eats\",\"eaten\",\"writing\",\"writes\",\"programming\",\"programs\",\"history\",\"finally\",\"finalized\"]\n",
    "for word in words:\n",
    "    print(word+\"---->\"+wnl().lemmatize(word,pos='v'))\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31c50fd8-f27f-444f-8e8d-220bd07e9135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n",
      "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n",
      "['The', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazy', 'dog', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ariji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ariji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ariji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# example 2 : tokenization , POS tagging and lemmatization of a paragraph\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Define the paragraph\n",
    "paragraph = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Tokenize the paragraph and get part-of-speech tags\n",
    "tokens = nltk.word_tokenize(paragraph)\n",
    "print(tokens)\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "print(pos_tags)\n",
    "\n",
    "# Create a WordNetLemmatizer object\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize each word based on its part-of-speech tag\n",
    "lemmatized_words = []\n",
    "for word, pos_tag in pos_tags:\n",
    "  tag = {\n",
    "      'N': 'n',\n",
    "      'V': 'v',\n",
    "      'A': 'a',\n",
    "      'R': 'r'\n",
    "  }.get(pos_tag[0], 'n')  # Default to noun if tag is unknown\n",
    "  lemma = lemmatizer.lemmatize(word, pos=tag)\n",
    "  lemmatized_words.append(lemma)\n",
    "\n",
    "# Print the lemmatized words\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c3fd6e3-cdd5-494b-9dab-0041544f0ef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# detailed example\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfa3cc15-9fe8-42ad-915d-2809ec865d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>txt145</td>\n",
       "      <td>The software had a steep learning curve at fir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>txt327</td>\n",
       "      <td>I'm really impressed with the user interface o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>txt209</td>\n",
       "      <td>The latest update to the software fixed severa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>txt825</td>\n",
       "      <td>I encountered a few glitches while using the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>txt878</td>\n",
       "      <td>I was skeptical about trying the software init...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  review_id                                               text\n",
       "0    txt145  The software had a steep learning curve at fir...\n",
       "1    txt327  I'm really impressed with the user interface o...\n",
       "2    txt209  The latest update to the software fixed severa...\n",
       "3    txt825  I encountered a few glitches while using the s...\n",
       "4    txt878  I was skeptical about trying the software init..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the necessary dataset\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/ariji/OneDrive/Desktop/Data/reviews.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e73cd0a7-6cf8-4836-b6bb-7e449defa391",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk_stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def lemmatize_with_nltk(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [token for token in tokens if token.isalpha() and token not in nltk_stop_words]\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "    return lemmatized_text\n",
    "    \n",
    "def lemmatize_with_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc if not token.is_stop and token.text.isalpha()]\n",
    "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fa9f3bc-5a81-4429-b4c8-04137e422622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Lemmatized Text:\n",
      "0     The software steep learning curve first I star...\n",
      "1     I really impressed user interface software It ...\n",
      "2     The latest update software fixed several bug i...\n",
      "3     I encountered glitch using software customer s...\n",
      "4     I skeptical trying software initially turned p...\n",
      "5     The analytics feature provided u valuable insi...\n",
      "6     I appreciate regular update software receives ...\n",
      "7     I attended training session software greatly i...\n",
      "8     The software documentation could comprehensive...\n",
      "9     I recommended software colleague due excellent...\n",
      "10    The software integration plugins expanded func...\n",
      "11    I looking forward upcoming release software pr...\n",
      "12    The user community active supportive making ea...\n",
      "13    I using software I consistently impressed stab...\n",
      "14    The user interface could use modernization fee...\n",
      "15           I went run software good job mapping route\n",
      "Name: lemmatized_text_nltk, dtype: object\n",
      "\n",
      "SpaCy Lemmatized Text:\n",
      "0     software steep learn curve start appreciate po...\n",
      "1     impressed user interface software intuitive ea...\n",
      "2     late update software fix bug improve overall p...\n",
      "3     encounter glitch software customer support qui...\n",
      "4     skeptical try software initially turn game cha...\n",
      "5     analytic feature provide valuable insight guid...\n",
      "6     appreciate regular update software receive bri...\n",
      "7     attend training session software greatly impro...\n",
      "8     software documentation comprehensive feature e...\n",
      "9     recommend software colleague excellent project...\n",
      "10    software integration party plugin expand funct...\n",
      "11    look forward upcoming release software promise...\n",
      "12    user community active supportive make easy tro...\n",
      "13    software consistently impress stability reliab...\n",
      "14    user interface use modernization feel somewhat...\n",
      "15                   go run software good job map route\n",
      "Name: lemmatized_text_spacy, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['lemmatized_text_nltk'] = df['text'].apply(lemmatize_with_nltk)\n",
    "df['lemmatized_text_spacy'] = df['text'].apply(lemmatize_with_spacy)\n",
    "print(\"NLTK Lemmatized Text:\")\n",
    "print(df['lemmatized_text_nltk'])\n",
    "print(\"\\nSpaCy Lemmatized Text:\")\n",
    "print(df['lemmatized_text_spacy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e78275-22cb-41d7-a738-aa652278b41f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7125bd7d-6ba5-4b18-8412-31c86d4df688",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
