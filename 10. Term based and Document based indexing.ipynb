{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8305303d-1772-4a40-a4b7-b5570ff60d77",
   "metadata": {},
   "source": [
    "# Term based Indexing\n",
    "\n",
    "Term-based indexing involves indexing documents based on individual terms or words that appear in the documents. By associating each term with a list\n",
    "of document identifiers where it occurs, we efficiently retrieve documents based on specific query terms. One of the advantages of term-based indexing \n",
    "is its fast and efficient retrieval of relevant documents containing the query terms. However, this type of indexing can be memory-intensive as it also\n",
    "requires the application of text preprocessing steps like tokenization, normalization, and stemming to handle variations in term spellings or word forms.\n",
    "In the example below, let’s see how to apply term-based indexing using Python. We’ll read reviews from a CSV file, tokenize them into words, remove\n",
    "common English stopwords, and then create an inverted index that maps each term to the list of review IDs in which it appears.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcc50065-363b-4987-b7d2-92f6ed8e4fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52f78291-e520-48e5-9c5c-76b54433fc22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>txt145</td>\n",
       "      <td>The software had a steep learning curve at fir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>txt327</td>\n",
       "      <td>I'm really impressed with the user interface o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>txt209</td>\n",
       "      <td>The latest update to the software fixed severa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>txt825</td>\n",
       "      <td>I encountered a few glitches while using the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>txt878</td>\n",
       "      <td>I was skeptical about trying the software init...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  review_id                                               text\n",
       "0    txt145  The software had a steep learning curve at fir...\n",
       "1    txt327  I'm really impressed with the user interface o...\n",
       "2    txt209  The latest update to the software fixed severa...\n",
       "3    txt825  I encountered a few glitches while using the s...\n",
       "4    txt878  I was skeptical about trying the software init..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the necessary dataset\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/ariji/OneDrive/Desktop/Data/reviews.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "603a4224-e3f6-4105-9e4f-3155198469d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term: software\n",
      "  Document ID: txt145, Positions: [0]\n",
      "  Document ID: txt327, Positions: [5]\n",
      "  Document ID: txt209, Positions: [2]\n",
      "  Document ID: txt825, Positions: [3]\n",
      "  Document ID: txt878, Positions: [2]\n",
      "  Document ID: txt718, Positions: [3]\n",
      "  Document ID: txt316, Positions: [3]\n",
      "  Document ID: txt247, Positions: [0]\n",
      "  Document ID: txt515, Positions: [2]\n",
      "  Document ID: txt913, Positions: [0]\n",
      "  Document ID: txt341, Positions: [5]\n",
      "  Document ID: txt688, Positions: [2]\n",
      "  Document ID: txt137, Positions: [2]\n",
      "Term: steep\n",
      "  Document ID: txt145, Positions: [1]\n",
      "Term: learning\n",
      "  Document ID: txt145, Positions: [2]\n",
      "Term: curve\n",
      "  Document ID: txt145, Positions: [3]\n",
      "Term: first\n",
      "  Document ID: txt145, Positions: [4]\n",
      "Term: ,\n",
      "  Document ID: txt145, Positions: [5, 6]\n",
      "  Document ID: txt825, Positions: [4]\n",
      "  Document ID: txt878, Positions: [4]\n",
      "  Document ID: txt718, Positions: [5]\n",
      "  Document ID: txt316, Positions: [4]\n",
      "  Document ID: txt247, Positions: [4]\n",
      "  Document ID: txt341, Positions: [6]\n",
      "  Document ID: txt943, Positions: [4]\n",
      "  Document ID: txt688, Positions: [3]\n",
      "  Document ID: txt136, Positions: [5]\n",
      "Term: started\n",
      "  Document ID: txt145, Positions: [7]\n",
      "Term: appreciate\n",
      "  Document ID: txt145, Positions: [8]\n",
      "  Document ID: txt718, Positions: [0]\n",
      "Term: powerful\n",
      "  Document ID: txt145, Positions: [9]\n",
      "Term: features\n",
      "  Document ID: txt145, Positions: [10]\n",
      "  Document ID: txt933, Positions: [1]\n",
      "  Document ID: txt718, Positions: [10]\n",
      "  Document ID: txt247, Positions: [5]\n",
      "Term: .\n",
      "  Document ID: txt145, Positions: [11]\n",
      "  Document ID: txt327, Positions: [6, 11]\n",
      "  Document ID: txt209, Positions: [9]\n",
      "  Document ID: txt825, Positions: [10]\n",
      "  Document ID: txt878, Positions: [8]\n",
      "  Document ID: txt933, Positions: [8]\n",
      "  Document ID: txt718, Positions: [11]\n",
      "  Document ID: txt316, Positions: [10]\n",
      "  Document ID: txt247, Positions: [8]\n",
      "  Document ID: txt515, Positions: [9]\n",
      "  Document ID: txt913, Positions: [8]\n",
      "  Document ID: txt341, Positions: [11]\n",
      "  Document ID: txt943, Positions: [11]\n",
      "  Document ID: txt688, Positions: [9]\n",
      "  Document ID: txt136, Positions: [11]\n",
      "  Document ID: txt137, Positions: [7]\n",
      "Term: 'm\n",
      "  Document ID: txt327, Positions: [0]\n",
      "  Document ID: txt341, Positions: [0]\n",
      "  Document ID: txt688, Positions: [4]\n",
      "Term: really\n",
      "  Document ID: txt327, Positions: [1]\n",
      "Term: impressed\n",
      "  Document ID: txt327, Positions: [2]\n",
      "  Document ID: txt688, Positions: [6]\n",
      "Term: user\n",
      "  Document ID: txt327, Positions: [3]\n",
      "  Document ID: txt943, Positions: [0]\n",
      "  Document ID: txt136, Positions: [0]\n",
      "Term: interface\n",
      "  Document ID: txt327, Positions: [4]\n",
      "  Document ID: txt136, Positions: [1]\n",
      "Term: 's\n",
      "  Document ID: txt327, Positions: [7]\n",
      "Term: intuitive\n",
      "  Document ID: txt327, Positions: [8]\n",
      "Term: easy\n",
      "  Document ID: txt327, Positions: [9]\n",
      "Term: navigate\n",
      "  Document ID: txt327, Positions: [10]\n",
      "Term: latest\n",
      "  Document ID: txt209, Positions: [0]\n",
      "Term: update\n",
      "  Document ID: txt209, Positions: [1]\n",
      "Term: fixed\n",
      "  Document ID: txt209, Positions: [3]\n",
      "Term: several\n",
      "  Document ID: txt209, Positions: [4]\n",
      "Term: bugs\n",
      "  Document ID: txt209, Positions: [5]\n",
      "Term: improved\n",
      "  Document ID: txt209, Positions: [6]\n",
      "  Document ID: txt316, Positions: [6]\n",
      "Term: overall\n",
      "  Document ID: txt209, Positions: [7]\n",
      "Term: performance\n",
      "  Document ID: txt209, Positions: [8]\n",
      "Term: encountered\n",
      "  Document ID: txt825, Positions: [0]\n",
      "Term: glitches\n",
      "  Document ID: txt825, Positions: [1]\n",
      "Term: using\n",
      "  Document ID: txt825, Positions: [2]\n",
      "  Document ID: txt688, Positions: [1]\n",
      "Term: customer\n",
      "  Document ID: txt825, Positions: [5]\n",
      "Term: support\n",
      "  Document ID: txt825, Positions: [6]\n",
      "Term: quick\n",
      "  Document ID: txt825, Positions: [7]\n",
      "Term: help\n",
      "  Document ID: txt825, Positions: [8]\n",
      "Term: resolve\n",
      "  Document ID: txt825, Positions: [9]\n",
      "Term: skeptical\n",
      "  Document ID: txt878, Positions: [0]\n",
      "Term: trying\n",
      "  Document ID: txt878, Positions: [1]\n",
      "Term: initially\n",
      "  Document ID: txt878, Positions: [3]\n",
      "Term: turned\n",
      "  Document ID: txt878, Positions: [5]\n",
      "Term: game-changer\n",
      "  Document ID: txt878, Positions: [6]\n",
      "Term: productivity\n",
      "  Document ID: txt878, Positions: [7]\n",
      "Term: analytics\n",
      "  Document ID: txt933, Positions: [0]\n",
      "Term: provided\n",
      "  Document ID: txt933, Positions: [2]\n",
      "Term: us\n",
      "  Document ID: txt933, Positions: [3]\n",
      "Term: valuable\n",
      "  Document ID: txt933, Positions: [4]\n",
      "Term: insights\n",
      "  Document ID: txt933, Positions: [5]\n",
      "  Document ID: txt943, Positions: [10]\n",
      "Term: guided\n",
      "  Document ID: txt933, Positions: [6]\n",
      "Term: decision-making\n",
      "  Document ID: txt933, Positions: [7]\n",
      "Term: regular\n",
      "  Document ID: txt718, Positions: [1]\n",
      "Term: updates\n",
      "  Document ID: txt718, Positions: [2]\n",
      "Term: receives\n",
      "  Document ID: txt718, Positions: [4]\n",
      "Term: often\n",
      "  Document ID: txt718, Positions: [6]\n",
      "Term: bring\n",
      "  Document ID: txt718, Positions: [7]\n",
      "Term: new\n",
      "  Document ID: txt718, Positions: [8]\n",
      "Term: useful\n",
      "  Document ID: txt718, Positions: [9]\n",
      "Term: attended\n",
      "  Document ID: txt316, Positions: [0]\n",
      "Term: training\n",
      "  Document ID: txt316, Positions: [1]\n",
      "Term: session\n",
      "  Document ID: txt316, Positions: [2]\n",
      "Term: greatly\n",
      "  Document ID: txt316, Positions: [5]\n",
      "Term: understanding\n",
      "  Document ID: txt316, Positions: [7]\n",
      "Term: advanced\n",
      "  Document ID: txt316, Positions: [8]\n",
      "Term: functionalities\n",
      "  Document ID: txt316, Positions: [9]\n",
      "Term: documentation\n",
      "  Document ID: txt247, Positions: [1]\n",
      "Term: could\n",
      "  Document ID: txt247, Positions: [2]\n",
      "  Document ID: txt136, Positions: [2]\n",
      "Term: comprehensive\n",
      "  Document ID: txt247, Positions: [3]\n",
      "Term: well\n",
      "  Document ID: txt247, Positions: [6]\n",
      "Term: explained\n",
      "  Document ID: txt247, Positions: [7]\n",
      "Term: 've\n",
      "  Document ID: txt515, Positions: [0]\n",
      "  Document ID: txt688, Positions: [0]\n",
      "Term: recommended\n",
      "  Document ID: txt515, Positions: [1]\n",
      "Term: colleagues\n",
      "  Document ID: txt515, Positions: [3]\n",
      "Term: due\n",
      "  Document ID: txt515, Positions: [4]\n",
      "Term: excellent\n",
      "  Document ID: txt515, Positions: [5]\n",
      "Term: project\n",
      "  Document ID: txt515, Positions: [6]\n",
      "Term: management\n",
      "  Document ID: txt515, Positions: [7]\n",
      "Term: capabilities\n",
      "  Document ID: txt515, Positions: [8]\n",
      "Term: integration\n",
      "  Document ID: txt913, Positions: [1]\n",
      "Term: third-party\n",
      "  Document ID: txt913, Positions: [2]\n",
      "Term: plugins\n",
      "  Document ID: txt913, Positions: [3]\n",
      "Term: expanded\n",
      "  Document ID: txt913, Positions: [4]\n",
      "Term: functionality\n",
      "  Document ID: txt913, Positions: [5]\n",
      "Term: made\n",
      "  Document ID: txt913, Positions: [6]\n",
      "Term: versatile\n",
      "  Document ID: txt913, Positions: [7]\n",
      "Term: looking\n",
      "  Document ID: txt341, Positions: [1]\n",
      "Term: forward\n",
      "  Document ID: txt341, Positions: [2]\n",
      "Term: upcoming\n",
      "  Document ID: txt341, Positions: [3]\n",
      "Term: release\n",
      "  Document ID: txt341, Positions: [4]\n",
      "Term: promises\n",
      "  Document ID: txt341, Positions: [7]\n",
      "Term: address\n",
      "  Document ID: txt341, Positions: [8]\n",
      "Term: current\n",
      "  Document ID: txt341, Positions: [9]\n",
      "Term: limitations\n",
      "  Document ID: txt341, Positions: [10]\n",
      "Term: community\n",
      "  Document ID: txt943, Positions: [1]\n",
      "Term: active\n",
      "  Document ID: txt943, Positions: [2]\n",
      "Term: supportive\n",
      "  Document ID: txt943, Positions: [3]\n",
      "Term: making\n",
      "  Document ID: txt943, Positions: [5]\n",
      "Term: easier\n",
      "  Document ID: txt943, Positions: [6]\n",
      "Term: troubleshoot\n",
      "  Document ID: txt943, Positions: [7]\n",
      "Term: issues\n",
      "  Document ID: txt943, Positions: [8]\n",
      "Term: share\n",
      "  Document ID: txt943, Positions: [9]\n",
      "Term: consistently\n",
      "  Document ID: txt688, Positions: [5]\n",
      "Term: stability\n",
      "  Document ID: txt688, Positions: [7]\n",
      "Term: reliability\n",
      "  Document ID: txt688, Positions: [8]\n",
      "Term: use\n",
      "  Document ID: txt136, Positions: [3]\n",
      "Term: modernization\n",
      "  Document ID: txt136, Positions: [4]\n",
      "Term: feels\n",
      "  Document ID: txt136, Positions: [6]\n",
      "Term: somewhat\n",
      "  Document ID: txt136, Positions: [7]\n",
      "Term: outdated\n",
      "  Document ID: txt136, Positions: [8]\n",
      "Term: compared\n",
      "  Document ID: txt136, Positions: [9]\n",
      "Term: competitors\n",
      "  Document ID: txt136, Positions: [10]\n",
      "Term: went\n",
      "  Document ID: txt137, Positions: [0]\n",
      "Term: run\n",
      "  Document ID: txt137, Positions: [1]\n",
      "Term: good\n",
      "  Document ID: txt137, Positions: [3]\n",
      "Term: job\n",
      "  Document ID: txt137, Positions: [4]\n",
      "Term: mapping\n",
      "  Document ID: txt137, Positions: [5]\n",
      "Term: route\n",
      "  Document ID: txt137, Positions: [6]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "apply a lambda function to tokenize each review text and then convert it to lowercase using word_tokenize. We save the result in a new tokens column.\n",
    "\n",
    "initialize a set named stop_words with common English stopwords from the stopwords.words('english') list and further process the tokens column by \n",
    "applying another lambda function to filter out any stopwords from the tokenized text.\n",
    "\n",
    "We initialize a nested defaultdict object called term_index. This data structure will store term-document positions, allowing us to quickly look up \n",
    "where terms appear in each document.\n",
    "\n",
    "We iterate over rows in the df DataFrame using the itertuples() method, extracting the review_id and tokens columns. This loop allows us to process \n",
    "each document’s tokens and associate them with document IDs. Within the loop, we further iterate over the tokens and their positions within the \n",
    "document. For each term in a document, we append its position to the term_index dictionary under the corresponding term and document ID.\n",
    "\n",
    "Finally, we print the results. For each term in term_index, we print the term itself, and for each document in which the term appears, we print the\n",
    "document ID and the positions of the term within that document.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "df['tokens'] = df['text'].apply(lambda text: word_tokenize(text.lower())) \n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['tokens'] = df['tokens'].apply(lambda tokens: [token for token in tokens if token not in stop_words])\n",
    "term_index = defaultdict(lambda: defaultdict(list))\n",
    "for idx, tokens in df[['review_id', 'tokens']].itertuples(index=False):\n",
    "    for position, term in enumerate(tokens):\n",
    "        term_index[term][idx].append(position)\n",
    "for term, doc_positions in term_index.items():\n",
    "    print(f\"Term: {term}\")\n",
    "    for doc_id, positions in doc_positions.items():\n",
    "        print(f\"  Document ID: {doc_id}, Positions: {positions}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44b23d31-9b8f-4ddc-9234-9a01cc5b07f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The software had a steep learning curve at first, but after a while, I started to appreciate its powerful features. I'm really impressed with the user interface of the software. It's intuitive and easy to navigate. The latest update to the software fixed several bugs and improved its overall performance. I encountered a few glitches while using the software, but the customer support was quick to help me resolve them. I was skeptical about trying the software initially, but it turned out to be a game-changer for our productivity. The analytics features have provided us with valuable insights that have guided our decision-making. I appreciate the regular updates that the software receives, as they often bring new and useful features. I attended a training session for the software, and it greatly improved my understanding of its advanced functionalities. The software documentation could be more comprehensive, as some features are not well explained. I've recommended the software to colleagues due to its excellent project management capabilities. The software integration with third-party plugins has expanded its functionality and made it more versatile. I'm looking forward to the upcoming release of the software, which promises to address some of the current limitations. The user community is active and supportive, making it easier to troubleshoot issues and share insights. I've been using the software for a while now, and I'm consistently impressed by its stability and reliability. The user interface could use some modernization, as it feels somewhat outdated compared to competitors. I went for a run and the software did a good job of mapping the route.\n"
     ]
    }
   ],
   "source": [
    "text = \" \".join(df['text'])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16e7f745-c8f9-4515-89c9-03536982fdc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>txt145</td>\n",
       "      <td>The software had a steep learning curve at fir...</td>\n",
       "      <td>[software, steep, learning, curve, first, ,, ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>txt327</td>\n",
       "      <td>I'm really impressed with the user interface o...</td>\n",
       "      <td>['m, really, impressed, user, interface, softw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>txt209</td>\n",
       "      <td>The latest update to the software fixed severa...</td>\n",
       "      <td>[latest, update, software, fixed, several, bug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>txt825</td>\n",
       "      <td>I encountered a few glitches while using the s...</td>\n",
       "      <td>[encountered, glitches, using, software, ,, cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>txt878</td>\n",
       "      <td>I was skeptical about trying the software init...</td>\n",
       "      <td>[skeptical, trying, software, initially, ,, tu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>txt933</td>\n",
       "      <td>The analytics features have provided us with v...</td>\n",
       "      <td>[analytics, features, provided, us, valuable, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>txt718</td>\n",
       "      <td>I appreciate the regular updates that the soft...</td>\n",
       "      <td>[appreciate, regular, updates, software, recei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>txt316</td>\n",
       "      <td>I attended a training session for the software...</td>\n",
       "      <td>[attended, training, session, software, ,, gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>txt247</td>\n",
       "      <td>The software documentation could be more compr...</td>\n",
       "      <td>[software, documentation, could, comprehensive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>txt515</td>\n",
       "      <td>I've recommended the software to colleagues du...</td>\n",
       "      <td>['ve, recommended, software, colleagues, due, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>txt913</td>\n",
       "      <td>The software integration with third-party plug...</td>\n",
       "      <td>[software, integration, third-party, plugins, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>txt341</td>\n",
       "      <td>I'm looking forward to the upcoming release of...</td>\n",
       "      <td>['m, looking, forward, upcoming, release, soft...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>txt943</td>\n",
       "      <td>The user community is active and supportive, m...</td>\n",
       "      <td>[user, community, active, supportive, ,, makin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>txt688</td>\n",
       "      <td>I've been using the software for a while now, ...</td>\n",
       "      <td>['ve, using, software, ,, 'm, consistently, im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>txt136</td>\n",
       "      <td>The user interface could use some modernizatio...</td>\n",
       "      <td>[user, interface, could, use, modernization, ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>txt137</td>\n",
       "      <td>I went for a run and the software did a good j...</td>\n",
       "      <td>[went, run, software, good, job, mapping, rout...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_id                                               text  \\\n",
       "0     txt145  The software had a steep learning curve at fir...   \n",
       "1     txt327  I'm really impressed with the user interface o...   \n",
       "2     txt209  The latest update to the software fixed severa...   \n",
       "3     txt825  I encountered a few glitches while using the s...   \n",
       "4     txt878  I was skeptical about trying the software init...   \n",
       "5     txt933  The analytics features have provided us with v...   \n",
       "6     txt718  I appreciate the regular updates that the soft...   \n",
       "7     txt316  I attended a training session for the software...   \n",
       "8     txt247  The software documentation could be more compr...   \n",
       "9     txt515  I've recommended the software to colleagues du...   \n",
       "10    txt913  The software integration with third-party plug...   \n",
       "11    txt341  I'm looking forward to the upcoming release of...   \n",
       "12    txt943  The user community is active and supportive, m...   \n",
       "13    txt688  I've been using the software for a while now, ...   \n",
       "14    txt136  The user interface could use some modernizatio...   \n",
       "15    txt137  I went for a run and the software did a good j...   \n",
       "\n",
       "                                               tokens  \n",
       "0   [software, steep, learning, curve, first, ,, ,...  \n",
       "1   ['m, really, impressed, user, interface, softw...  \n",
       "2   [latest, update, software, fixed, several, bug...  \n",
       "3   [encountered, glitches, using, software, ,, cu...  \n",
       "4   [skeptical, trying, software, initially, ,, tu...  \n",
       "5   [analytics, features, provided, us, valuable, ...  \n",
       "6   [appreciate, regular, updates, software, recei...  \n",
       "7   [attended, training, session, software, ,, gre...  \n",
       "8   [software, documentation, could, comprehensive...  \n",
       "9   ['ve, recommended, software, colleagues, due, ...  \n",
       "10  [software, integration, third-party, plugins, ...  \n",
       "11  ['m, looking, forward, upcoming, release, soft...  \n",
       "12  [user, community, active, supportive, ,, makin...  \n",
       "13  ['ve, using, software, ,, 'm, consistently, im...  \n",
       "14  [user, interface, could, use, modernization, ,...  \n",
       "15  [went, run, software, good, job, mapping, rout...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c18f409-d3c7-4036-844d-6728ca373567",
   "metadata": {},
   "source": [
    "# Document Based Indexing\n",
    "\n",
    "Document-based indexing involves indexing documents based on their overall content, metadata, structure, or other document-level features. It varies\n",
    "from term-based indexing in that it indexes documents as a whole rather than preprocessing specific words or keywords. It’s a valuable technique for\n",
    "text classification, summarization, and retrieval based on document metadata (e.g., author, title, date), document structure\n",
    "(e.g., sections, paragraphs), or other document-level properties. Let’s see how to apply document-based indexing in the \n",
    "code example below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "109cece4-3253-41a6-879a-efc10e2a7951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>txt145</td>\n",
       "      <td>The software had a steep learning curve at fir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>txt327</td>\n",
       "      <td>I'm really impressed with the user interface o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>txt209</td>\n",
       "      <td>The latest update to the software fixed severa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>txt825</td>\n",
       "      <td>I encountered a few glitches while using the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>txt878</td>\n",
       "      <td>I was skeptical about trying the software init...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  review_id                                               text\n",
       "0    txt145  The software had a steep learning curve at fir...\n",
       "1    txt327  I'm really impressed with the user interface o...\n",
       "2    txt209  The latest update to the software fixed severa...\n",
       "3    txt825  I encountered a few glitches while using the s...\n",
       "4    txt878  I was skeptical about trying the software init..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv(\"C:/Users/ariji/OneDrive/Desktop/Data/reviews.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da4b22f4-90c0-4ab7-a833-e4d33d128af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReviewID: txt145 -> DocumentID: 0\n",
      "ReviewID: txt327 -> DocumentID: 1\n",
      "ReviewID: txt209 -> DocumentID: 2\n",
      "ReviewID: txt825 -> DocumentID: 3\n",
      "ReviewID: txt878 -> DocumentID: 4\n",
      "ReviewID: txt933 -> DocumentID: 5\n",
      "ReviewID: txt718 -> DocumentID: 6\n",
      "ReviewID: txt316 -> DocumentID: 7\n",
      "ReviewID: txt247 -> DocumentID: 8\n",
      "ReviewID: txt515 -> DocumentID: 9\n",
      "ReviewID: txt913 -> DocumentID: 10\n",
      "ReviewID: txt341 -> DocumentID: 11\n",
      "ReviewID: txt943 -> DocumentID: 12\n",
      "ReviewID: txt688 -> DocumentID: 13\n",
      "ReviewID: txt136 -> DocumentID: 14\n",
      "ReviewID: txt137 -> DocumentID: 15\n",
      "\n",
      "Requested Reviews:\n",
      "ReviewID: rv1315 -> Review not found\n",
      "ReviewID: rv2087 -> Review not found\n",
      "ReviewID: rv6898 -> Review not found\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We create a pandas Series named document_index using the pd.Series constructor and the DataFrame’s index as the data, and the review_id column as the\n",
    "index for the Series. This essentially creates a mapping from the review_id values to their corresponding DataFrame indexes. We then convert the\n",
    "document_index Series into a dictionary using the to_dict() method. This step facilitates the easy lookup of DataFrame indexes based on review_id.\n",
    "\n",
    "We start a for loop to iterate through each key-value pair in the document_index dictionary. Inside the loop, we unpack the key-value pair into the \n",
    "variables review_id and document_id, where review_id represents the review_id value from the original DataFrame, and document_id represents the\n",
    "corresponding DataFrame index. We then display each loop iteration’s review and document IDs using the f-string format.\n",
    "\n",
    "We then create a list named requested_review_ids containing specific review_id values that we want to retrieve. We also print a separator line to \n",
    "indicate the beginning of the requested reviews display.\n",
    "\n",
    "To check whether the review_id values exist, we create a for loop to iterate through each requested_id in the requested_review_ids list. Inside the loop,\n",
    "\n",
    "We use a conditional if statement to check if the current requested_id is present in the document_index dictionary.\n",
    "\n",
    "If requested_id is found in the document_index:\n",
    "\n",
    "We retrieve the corresponding document_id using dictionary indexing.\n",
    "\n",
    "We then use the loc accessor to extract the review text from the df DataFrame using document_id.\n",
    "\n",
    "We similarly use the loc accessor to extract the sentiment value from the df DataFrame using document_id.\n",
    "\n",
    "Later, we use the print() function to display the retrieved review information, including review_id, document_id, sentiment, and review_text.\n",
    "\n",
    "If requested_id is not found in the document_index, we use the print() function to display a message indicating that the requested review was not found.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "document_index = pd.Series(df.index, index=df['review_id']).to_dict()\n",
    "for review_id, document_id in document_index.items():\n",
    "    print(f\"ReviewID: {review_id} -> DocumentID: {document_id}\") \n",
    "requested_review_ids = [\"rv1315\", \"rv2087\", \"rv6898\"] \n",
    "print(\"\\nRequested Reviews:\")\n",
    "for requested_id in requested_review_ids:\n",
    "    if requested_id in document_index:\n",
    "        document_id = document_index[requested_id]\n",
    "        review_text = df.loc[document_id, 'review']\n",
    "        sentiment = df.loc[document_id, 'sentiment']\n",
    "        print(f\"ReviewID: {requested_id} -> DocumentID: {document_id} -> Sentiment: {sentiment} -> Review Text: {review_text}\")\n",
    "    else:\n",
    "        print(f\"ReviewID: {requested_id} -> Review not found\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a61b3ed-12f4-4eef-83bc-dd4598ed6236",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f45ac3-9160-4cdc-9166-3a406df6035c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ff8a2e-6d54-420d-abfe-f2f3400297b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f84c68e-a818-4e9d-9de3-541c47477489",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb69477-f629-4f79-be9a-7eb97944c2ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a93647-2eec-479a-8b6e-0d863aa57168",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaf50d8-576b-4081-be19-debd6845ea49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058fa79e-f306-41a7-b701-216ba876c512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543e7533-64c2-4dcf-b7b4-d957a45e6081",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38fd0ed-4b8a-4566-a814-b8ea74756880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec84d644-ab32-486b-a0bf-9d9f9dabef93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf09fef2-8eb6-4652-9007-3506e5a9807d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8259e638-223e-4313-b3ae-99d7d470ba50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3855571b-6a76-4aad-b309-5cc480004dc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df2f5c5-b00b-401b-a323-9b0ee2aa287e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00252058-f1a0-44c7-8c33-d58bd9a12fdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea020ce2-bd1a-4f12-b42c-2b39a62b9ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
